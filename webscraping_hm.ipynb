{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4a4eb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ imports ###############################\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import logging\n",
    "\n",
    "############################## Data Colletion ##########################\n",
    "def data_colletion(url, headers):\n",
    "    # Request to URL\n",
    "    page = requests.get( url, headers=headers )\n",
    "\n",
    "    # Beautiful soup object\n",
    "    soup = BeautifulSoup( page.text, 'html.parser' )\n",
    "\n",
    "    ######## Product Data #########\n",
    "    products = soup.find( 'ul', class_='products-listing small' )\n",
    "    product_list = products.find_all( 'article', class_='hm-product-item')\n",
    "\n",
    "    # product id\n",
    "    #data-article code é o ID do produto\n",
    "    product_id = [p.get( 'data-articlecode' ) for p in product_list]\n",
    "\n",
    "    # product category\n",
    "    product_category = [p.get( 'data-category' ) for p in product_list]\n",
    "\n",
    "    # product name\n",
    "    product_list = products.find_all( 'a', class_='link' )\n",
    "    product_name = [p.get_text() for p in product_list]\n",
    "\n",
    "    # price\n",
    "    product_list = products.find_all( 'span', class_='price regular' )\n",
    "    product_price = [p.get_text() for p in product_list]\n",
    "\n",
    "    #criação do dataframe\n",
    "    data = pd.DataFrame( [product_id, product_category, product_name,product_price] ).T\n",
    "    data.columns = ['product_id', 'product_category', 'product_name','product_price']\n",
    "\n",
    "    # scrapy datetime\n",
    "    data['scrapy_datetime'] = datetime.now().strftime( '%Y-%m-%d %H:%M:%S' )\n",
    "    \n",
    "    return data\n",
    "\n",
    "############################## Data Colletion by Product ##########################\n",
    "def data_colletion_by_product(data, headers):\n",
    "    #criando um df vazio \n",
    "    df_compositions = pd.DataFrame()\n",
    "\n",
    "    #lista vazia para colocar todos os nomes das colunas para que o df seja padronizado, se algum produto\n",
    "    #não tiver alguma coluna vai ficar vazio\n",
    "    aux = []\n",
    "\n",
    "    cols = ['Art. No.', 'Composition', 'Fit', 'color_id', 'style_id']\n",
    "    df_pattern = pd.DataFrame(columns=cols)\n",
    "\n",
    "    #parametros\n",
    "    #headers é um dicionário que vai dizer pra API da H&M que quem está fazendo a requisição\n",
    "    #é um browser e não um código python; é padrão\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "\n",
    "    #gerando todas as urls\n",
    "    for i in range(len(data)):\n",
    "        #API requests\n",
    "        url = 'https://www2.hm.com/en_us/productpage.' + data.loc[i,'product_id'] + '.html'\n",
    "        logger.debug('Product: %s ', url)\n",
    "\n",
    "        #para fazer requisição na API para puxar os dados html       \n",
    "        page = requests.get(url, headers=headers)\n",
    "\n",
    "        #BeautifulSoup objects\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "        ######## color name ########\n",
    "        product_list= soup.find_all('a',class_='filter-option miniature') + soup.find_all('a',class_='filter-option miniature active')\n",
    "\n",
    "        #percorrendo todas as cores do produto\n",
    "        color_name = [p.get('data-color') for p in product_list]\n",
    "\n",
    "        #procuct id\n",
    "        product_id = [p.get('data-articlecode') for p in product_list]\n",
    "\n",
    "        #criando o df\n",
    "        df_color = pd.DataFrame([product_id, color_name]).T\n",
    "        df_color.columns = ['product_id','color_name']\n",
    "\n",
    "        #loop que percorre todas as cores e coleta a composição de cada cor\n",
    "        for j in range(len(df_color)):\n",
    "            #API requests\n",
    "            url = 'https://www2.hm.com/en_us/productpage.' + df_color.loc[j,'product_id'] + '.html'\n",
    "            #url = 'https://www2.hm.com/en_us/productpage.1024256001.html'\n",
    "            logger.debug('Color: %s ', url)\n",
    "\n",
    "            #para fazer requisição na API para puxar os dados html       \n",
    "            page = requests.get(url, headers=headers)\n",
    "\n",
    "            #BeautifulSoup objects\n",
    "            soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "            ########Product Name ########\n",
    "            time.sleep(10)\n",
    "            product_name = soup.find_all('hm-product-name', {'id':'js-product-name' })\n",
    "            #product_name = soup.find_all('h1', {'class':'Heading-module--general__3HQET ProductName-module--productTitle__1T9f0 Heading-module--small__SFfSh' })\n",
    "            #product_name = soup.find_all('h1', class_='class=\"Heading-module--general__3HQET ProductName-module--productTitle__1T9f0 Heading-module--small__SFfSh')\n",
    "\n",
    "            product_name = product_name[0].get_text()\n",
    "            #print(product_name)\n",
    "\n",
    "            #########Product Price ########\n",
    "            product_price = soup.find_all('div', class_='primary-row product-item-price')\n",
    "            #regex para pegar o preço\n",
    "            product_price = re.findall(r'\\d+\\.?\\d+',product_price[0].get_text())[0]\n",
    "\n",
    "            ######## composition ########\n",
    "            product_composition_list = soup.find_all('div', class_='details-attributes-list-item')\n",
    "\n",
    "            product_composition_full = [list(filter(None,p.get_text().split('\\n'))) for p in product_composition_list]\n",
    "\n",
    "            #rename dataframe\n",
    "            df_composition = pd.DataFrame(product_composition_full).T\n",
    "            df_composition.columns = df_composition.iloc[0]\n",
    "\n",
    "            #add o product name e o price no df\n",
    "            df_composition['product_name'] = product_name\n",
    "            df_composition['product_price'] = product_price\n",
    "\n",
    "            #delete first row\n",
    "            df_composition = df_composition.iloc[1:].fillna(method='ffill')\n",
    "\n",
    "            #removendo pocket lining, shell e lining\n",
    "            df_composition['Composition'] = df_composition['Composition'].replace('Pocket lining: ','',regex=True)\n",
    "            df_composition['Composition'] = df_composition['Composition'].replace('Shell: ','',regex=True)\n",
    "            df_composition['Composition'] = df_composition['Composition'].replace('Lining: ','',regex=True)\n",
    "\n",
    "            #garantia que tenha a mesma quantidade de colunas\n",
    "            df_composition = pd.concat( [df_pattern, df_composition], axis=0 )\n",
    "\n",
    "            #Para arrumar o df que tem informações a mais que eu preciso\n",
    "            ex = pd.DataFrame(columns=['Fit','Composition','Art. No.'])\n",
    "            df_composition = pd.merge(ex, df_composition[['Fit','Composition','Art. No.']], how = 'right', on=['Fit','Composition','Art. No.'])\n",
    "            df_composition = df_composition.drop_duplicates()\n",
    "\n",
    "            # gereando o style id + color id\n",
    "            #df_composition['style_id'] = df_composition['Art. No.'].apply( lambda x: x[:-3] )\n",
    "            #df_composition['color_id'] = df_composition['Art. No.'].apply( lambda x: x[-3:] )\n",
    "\n",
    "            #renomeando as colunas\n",
    "            df_composition.columns = ['fit','composition','product_id']\n",
    "            df_composition['product_name'] = product_name\n",
    "            df_composition['product_price'] = product_price\n",
    "\n",
    "            #lista vazia para colocar todos os nomes das colunas para que o df seja padronizado, se algum produto\n",
    "            #não tiver alguma coluna vai ficar vazio\n",
    "            aux = aux + df_composition.columns.tolist()\n",
    "\n",
    "            #juntando data color + composition\n",
    "            df_composition = pd.merge(df_composition, df_color,how='left', on='product_id')\n",
    "\n",
    "            #todos os produtos\n",
    "            df_compositions = pd.concat([df_compositions, df_composition], axis = 0)\n",
    "\n",
    "\n",
    "    #Join Showroom data + details\n",
    "    df_compositions['style_id'] = df_compositions['product_id'].apply(lambda x: x[:-3])\n",
    "    df_compositions['color_id'] = df_compositions['product_id'].apply(lambda x: x[-3:])\n",
    "\n",
    "    #scrapy datetime\n",
    "    df_compositions['scrapy_datetime'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return df_compositions\n",
    "\n",
    "############################## Data Cleaning ##########################\n",
    "\n",
    "def data_cleaning(data_products):\n",
    "    #product_id\n",
    "    df_data = data_products.dropna(subset=['product_id'])\n",
    "\n",
    "    #product_name\n",
    "    df_data['product_name'] = df_data['product_name'].str.replace( '\\n', '' )\n",
    "    df_data['product_name'] = df_data['product_name'].str.replace( '\\t', '' )\n",
    "    df_data['product_name'] = df_data['product_name'].str.replace( ' ', '' )\n",
    "\n",
    "    #deixar minusculo e separado por _ igual o product_category\n",
    "    df_data['product_name'] = df_data['product_name'].apply( lambda x: x.replace(' ','_').lower())\n",
    "\n",
    "    #product_price\n",
    "    #substituir o $ por nada e colocar o tipo como float\n",
    "    df_data['product_price'] = df_data['product_price'].apply( lambda x: x.replace('$ ','')).astype(float)\n",
    "\n",
    "    #scrapy_datetime\n",
    "    #tranformar para o tipo data\n",
    "    #data['scrapy_datetime'] = pd.to_datetime(data['scrapy_datetime'], format = '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    #color_name    \n",
    "    #deixar minusculo e separado por _ quando não for nulo\n",
    "    df_data['color_name'] = df_data['color_name'].apply(lambda x: x.replace(' ','_').lower() if pd.notnull(x) else x)\n",
    "\n",
    "    #Fit \n",
    "    #deixar minusculo e separado por _ quando não for nulo\n",
    "    df_data['fit'] = df_data['fit'].apply(lambda x: x.replace(' ','_').lower() if pd.notnull(x) else x)\n",
    "\n",
    "    # size number\n",
    "    #df_data['size_number'] = df_data['size'].apply( lambda x: re.search( '\\d{3}cm',x ).group(0) if pd.notnull( x ) else x )\n",
    "    #df_data['size_number'] = df_data['size_number'].apply( lambda x: re.search('\\d+', x ).group(0) if pd.notnull( x ) else x )\n",
    "\n",
    "    # size model\n",
    "    #df_data['size_model'] = df_data['size'].str.extract( '(\\d+/\\\\d+)' )\n",
    "\n",
    "    ####### Composition #########\n",
    "    #quebrando a Composition na virgula e resetando o index\n",
    "    df1 = df_data['composition'].str.split(',', expand=True).reset_index(drop=True)\n",
    "\n",
    "    # cotton | polyester | spandex | \n",
    "    df_ref = pd.DataFrame( index=np.arange( len( df_data ) ),columns=['cotton','polyester', 'elastane', 'elasterell'] )\n",
    "\n",
    "    #criando as colunas com cada tipo de material\n",
    "    # ------ cotton -------\n",
    "    df_cotton_0 = df1.loc[df1[0].str.contains( 'Cotton', na=True ), 0]\n",
    "    df_cotton_0.name = 'cotton'\n",
    "\n",
    "    df_cotton_1 = df1.loc[df1[1].str.contains( 'Cotton', na=True ), 1]\n",
    "    df_cotton_1.name = 'cotton'\n",
    "\n",
    "    # combinando as colunas\n",
    "    df_cotton = df_cotton_0.combine_first( df_cotton_1 )\n",
    "    df_ref = pd.concat( [df_ref, df_cotton ], axis=1 )\n",
    "    df_ref = df_ref.iloc[:, ~df_ref.columns.duplicated( keep='last')]\n",
    "\n",
    "    # ------ polyester -------\n",
    "    df_polyester_0 = df1.loc[df1[0].str.contains( 'Polyester', na=True ), 0]\n",
    "    df_polyester_0.name = 'polyester'\n",
    "\n",
    "    df_polyester_1 = df1.loc[df1[1].str.contains( 'Polyester', na=True ), 1]\n",
    "    df_polyester_1.name = 'polyester'\n",
    "\n",
    "    # combine\n",
    "    df_polyester = df_polyester_0.combine_first( df_polyester_1 )\n",
    "    df_ref = pd.concat( [df_ref, df_polyester], axis=1 )\n",
    "    df_ref = df_ref.iloc[:, ~df_ref.columns.duplicated( keep='last') ]\n",
    "\n",
    "    # ------ spandex -------\n",
    "    df_spandex_0 = df1.loc[df1[0].str.contains( 'Spandex', na=True ), 0]\n",
    "    df_spandex_0.name = 'spandex'\n",
    "\n",
    "    df_spandex_1 = df1.loc[df1[1].str.contains( 'Spandex', na=True ), 1]\n",
    "    df_spandex_1.name = 'spandex'\n",
    "\n",
    "    # combine\n",
    "    df_spandex = df_spandex_0.combine_first( df_spandex_1 )\n",
    "    df_ref = pd.concat( [df_ref, df_spandex], axis=1 )\n",
    "    df_ref = df_ref.iloc[:, ~df_ref.columns.duplicated( keep='last') ]\n",
    "\n",
    "    # join que combina com o product_id\n",
    "    df_aux = pd.concat( [df_data['product_id'].reset_index(drop=True), df_ref],axis=1 )\n",
    "\n",
    "    #format composition data\n",
    "    #quero extrair só os numeros das colunas de composição\n",
    "    df_aux['cotton'] = df_aux['cotton'].apply(lambda x: int(re.search('\\d+',x).group(0))/100 if pd.notnull(x) else x)\n",
    "    df_aux['polyester'] = df_aux['polyester'].apply(lambda x: int(re.search('\\d+',x).group(0))/100 if pd.notnull(x) else x)\n",
    "    df_aux['spandex'] = df_aux['spandex'].apply(lambda x: int(re.search('\\d+',x).group(0))/100 if pd.notnull(x) else x)\n",
    "\n",
    "    # final join\n",
    "    #pegando o valor máximo no agrupamento e depois colocando 0 quando estiver vazio\n",
    "    df_aux = df_aux.groupby( 'product_id' ).max().reset_index().fillna( 0 )\n",
    "\n",
    "    #juntando os dfs\n",
    "    df_data = pd.merge( df_data, df_aux, on='product_id', how='left' )\n",
    "\n",
    "    # Drop columns\n",
    "    #df_data = df_data.drop(columns=['size', 'product_safety', 'composition'], axis=1 )\n",
    "    df_data = df_data.drop(columns=['composition'], axis=1 )\n",
    "\n",
    "    # Drop duplicates\n",
    "    df_data = df_data.drop_duplicates()\n",
    "    \n",
    "    return df_data\n",
    "\n",
    "############################## Data Insert ##########################\n",
    "def  data_insert(data_product_cleaned):\n",
    "#mudando a posição das colunas do df\n",
    "    data_insert = data_product_cleaned[['product_id',\n",
    "                            'style_id',\n",
    "                            'color_id',\n",
    "                            'product_name',\n",
    "                            'color_name',\n",
    "                            'fit',\n",
    "                            'product_price',\n",
    "                            'cotton',\n",
    "                            'polyester',\n",
    "                            'spandex',\n",
    "                            'scrapy_datetime'\n",
    "    ]]\n",
    "\n",
    "    #criando a conexão com o banco\n",
    "    conn = create_engine( 'sqlite:///database_hm.sqlite', echo=False )\n",
    "\n",
    "    #data insert\n",
    "    data_insert.to_sql( 'vitrine', con=conn, if_exists='append', index=False )\n",
    "    \n",
    "    return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #logging\n",
    "    path = 'C:/Users/laais/CDS_Python_do_DS_ao_DEV/'\n",
    "    if not os.path.exists(path + 'Logs'):\n",
    "        os.makedirs(path + 'Logs')\n",
    "        \n",
    "    logging.basicConfig(\n",
    "        filename = path + 'Logs/webscraping_hm.log',\n",
    "        level = logging.DEBUG,\n",
    "        format= '%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "        datefmt = '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # logger é o agente que vai fazer o logging, se deixar padrão vai aparecer root, mas colocar o nome da aplicação\n",
    "    logger = logging.getLogger('webscraping_hm')\n",
    "    \n",
    "    #parameters and constants\n",
    "    # parameters\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5),AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "\n",
    "    # URL\n",
    "    url = 'https://www2.hm.com/en_us/men/products/jeans.html'\n",
    "    \n",
    "    # data colletion\n",
    "    data = data_colletion(url, headers)\n",
    "    logger.info('data colletion done')\n",
    "    \n",
    "    # data colletion by product\n",
    "    data_product = data_colletion_by_product(data, headers)\n",
    "    logger.info('data colletion by product done')\n",
    "    \n",
    "    # data cleaning\n",
    "    data_product_cleaned = data_cleaning(data_product)\n",
    "    logger.info('data cleaning done')\n",
    "    \n",
    "    #data insertion\n",
    "    data_insert(data_product_cleaned)\n",
    "    logger.info('data insertion done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a83d68",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c9050e7",
   "metadata": {},
   "outputs": [],
   "source": [
    " conn = create_engine( 'sqlite:///database_hm.sqlite', echo=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f514815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>style_id</th>\n",
       "      <th>color_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>color_name</th>\n",
       "      <th>fit</th>\n",
       "      <th>product_price</th>\n",
       "      <th>cotton</th>\n",
       "      <th>polyester</th>\n",
       "      <th>spandex</th>\n",
       "      <th>scrapy_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024256002</td>\n",
       "      <td>1024256</td>\n",
       "      <td>002</td>\n",
       "      <td>slimjeans</td>\n",
       "      <td>light_denim_blue</td>\n",
       "      <td>slim_fit</td>\n",
       "      <td>19.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2022-09-14 10:03:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1024256003</td>\n",
       "      <td>1024256</td>\n",
       "      <td>003</td>\n",
       "      <td>slimjeans</td>\n",
       "      <td>light_denim_blue</td>\n",
       "      <td>slim_fit</td>\n",
       "      <td>19.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2022-09-14 10:03:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1024256004</td>\n",
       "      <td>1024256</td>\n",
       "      <td>004</td>\n",
       "      <td>slimjeans</td>\n",
       "      <td>denim_blue</td>\n",
       "      <td>slim_fit</td>\n",
       "      <td>19.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2022-09-14 10:03:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1024256005</td>\n",
       "      <td>1024256</td>\n",
       "      <td>005</td>\n",
       "      <td>slimjeans</td>\n",
       "      <td>dark_blue</td>\n",
       "      <td>slim_fit</td>\n",
       "      <td>19.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2022-09-14 10:03:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1024256006</td>\n",
       "      <td>1024256</td>\n",
       "      <td>006</td>\n",
       "      <td>slimjeans</td>\n",
       "      <td>dark_denim_blue</td>\n",
       "      <td>slim_fit</td>\n",
       "      <td>19.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2022-09-14 10:03:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id style_id color_id product_name        color_name       fit  \\\n",
       "0  1024256002  1024256      002    slimjeans  light_denim_blue  slim_fit   \n",
       "1  1024256003  1024256      003    slimjeans  light_denim_blue  slim_fit   \n",
       "2  1024256004  1024256      004    slimjeans        denim_blue  slim_fit   \n",
       "3  1024256005  1024256      005    slimjeans         dark_blue  slim_fit   \n",
       "4  1024256006  1024256      006    slimjeans   dark_denim_blue  slim_fit   \n",
       "\n",
       "   product_price  cotton  polyester  spandex      scrapy_datetime  \n",
       "0          19.99    0.99       0.65     0.01  2022-09-14 10:03:28  \n",
       "1          19.99    0.99       0.65     0.01  2022-09-14 10:03:28  \n",
       "2          19.99    0.99       0.65     0.01  2022-09-14 10:03:28  \n",
       "3          19.99    0.99       0.65     0.01  2022-09-14 10:03:28  \n",
       "4          19.99    1.00       0.00     0.01  2022-09-14 10:03:28  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM vitrine\n",
    "\"\"\"\n",
    "\n",
    "df1 = pd.read_sql_query(query, conn)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c3c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
